{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# get all paper links\n",
    "papers = []\n",
    "for year in range(1987, 2024):\n",
    "    response = requests.get(f'https://papers.nips.cc/paper_files/paper/{year}')\n",
    "    soup = bs4.BeautifulSoup(response.text, \"html.parser\")\n",
    "    links = soup.find_all('a')\n",
    "\n",
    "    papers_year = 0\n",
    "    for link in links:\n",
    "        if '/paper_files/paper/' in link.get('href'):\n",
    "            papers.append({\n",
    "                'link': f'https://papers.nips.cc' + link.get('href'),\n",
    "                'year': year,\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get paper titles, authors, and abstracts\n",
    "for i in range(len(papers)):\n",
    "    response = requests.get(papers[i]['link'])\n",
    "    soup = bs4.BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    papers[i]['title'] = soup.find_all('h4')[0].text\n",
    "    papers[i]['authors'] = soup.find_all('i')[-1].text\n",
    "\n",
    "    text = soup.get_text()\n",
    "    abstract = text[text.find('Abstract') + 8:text.find('Name Change Policy')].replace('\\n', ' ').strip()\n",
    "    papers[i]['abstract'] = abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to pickle\n",
    "df = pd.DataFrame.from_dict(papers)\n",
    "df.to_pickle('neurips.pkl')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
